{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fef0a707",
   "metadata": {},
   "source": [
    "# ICD Code Classifier from MIMIC-III Clinical Notes using NLP and Neural Networks\n",
    "\n",
    "This notebook demonstrates ICD code classification from clinical notes using the MIMIC-III dataset. It includes baseline models, transformer-based models, prompt learning, knowledge injection, hyperparameter search, and explainability. The codes in MIMIC-III are ICD-9; for ICD-10 tasks, you may map ICD-9 to ICD-10 using an external mapping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba527192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NourH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\NourH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab4703",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation (MIMIC-III)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3ad69d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Number of unique ICD codes: 390\n",
      "Total samples: 3816\n",
      "                                                     text icd_code  label\n",
      "286401  Admission Date: [**2111-2-18**]        Dischar...    76527    271\n",
      "453099  Admission Date:  [**2168-12-29**]             ...    V4986    382\n",
      "61587   Admission Date:  [**2127-12-16**]             ...    36201    111\n",
      "674347  Name:  [**Known lastname 1985**],[**Known firs...     3962    113\n",
      "467364  Admission Date:  [**2141-5-31**]              ...    42731    142\n"
     ]
    }
   ],
   "source": [
    "# Load MIMIC-III notes and diagnoses\n",
    "DATA_DIR = './mimic-iii-clinical-database-1.4/'\n",
    "notes = pd.read_csv(DATA_DIR + 'NOTEEVENTS.csv')\n",
    "diagnoses = pd.read_csv(DATA_DIR + 'DIAGNOSES_ICD.csv')\n",
    "admissions = pd.read_csv(DATA_DIR + 'ADMISSIONS.csv')\n",
    "\n",
    "# Merge to get notes and ICD-9 codes per admission\n",
    "notes = notes[notes['CATEGORY'] == 'Discharge summary']\n",
    "merged = notes.merge(admissions[['HADM_ID', 'SUBJECT_ID']], on=['HADM_ID', 'SUBJECT_ID'])\n",
    "merged = merged.merge(diagnoses[['HADM_ID', 'ICD9_CODE']], on='HADM_ID')\n",
    "\n",
    "df = merged[['TEXT', 'ICD9_CODE']].rename(columns={'TEXT': 'text', 'ICD9_CODE': 'icd_code'})\n",
    "\n",
    "\n",
    "# Only keep classes with at least 3 samples\n",
    "code_counts = df['icd_code'].value_counts()\n",
    "df = df[df['icd_code'].isin(code_counts[code_counts >= 3].index)]\n",
    "\n",
    "# Stratified sample if needed\n",
    "if len(df) > 2000:\n",
    "    df, _ = train_test_split(df, train_size=5000, stratify=df['icd_code'], random_state=SEED)\n",
    "\n",
    "# Remove classes with <3 samples again (in case sampling reduced some)\n",
    "code_counts = df['icd_code'].value_counts()\n",
    "df = df[df['icd_code'].isin(code_counts[code_counts >= 3].index)]\n",
    "\n",
    "# Encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['icd_code'])\n",
    "\n",
    "# Double-check\n",
    "print(df['label'].value_counts().min())  # Should be at least 3\n",
    "print(f\"Number of unique ICD codes: {df['label'].nunique()}\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ee95a",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d874f10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text icd_code  label\n",
      "286401  admission date: [**2111-2-18**]        dischar...    76527    271\n",
      "453099  admission date:  [**2168-12-29**]             ...    V4986    382\n",
      "61587   admission date:  [**2127-12-16**]             ...    36201    111\n",
      "674347  name:  [**known lastname 1985**],[**known firs...     3962    113\n",
      "467364  admission date:  [**2141-5-31**]              ...    42731    142\n",
      "Number of unique ICD codes: 390\n"
     ]
    }
   ],
   "source": [
    "# Basic text cleaning (customize as needed)\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "label2code = dict(zip(df['label'], df['icd_code']))\n",
    "code2label = dict(zip(df['icd_code'], df['label']))\n",
    "\n",
    "print(df[['text', 'icd_code', 'label']].head())\n",
    "print(f\"Number of unique ICD codes: {len(label2code)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "164d3b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2441, Val: 611, Test: 764\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=SEED, stratify=df['label'])\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=SEED, stratify=train_df['label'])\n",
    "print(f'Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9019fabf",
   "metadata": {},
   "source": [
    "## 3. Baseline: TF-IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff28d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       00845       0.00      0.00      0.00         2\n",
      "        0380       0.00      0.00      0.00         1\n",
      "       03811       0.00      0.00      0.00         1\n",
      "       03842       0.00      0.00      0.00         1\n",
      "        0389       0.00      0.00      0.00         6\n",
      "       04104       0.00      0.00      0.00         1\n",
      "       04111       0.00      0.00      0.00         1\n",
      "       04119       0.00      0.00      0.00         1\n",
      "        0413       0.00      0.00      0.00         1\n",
      "        0414       0.00      0.00      0.00         2\n",
      "        0417       0.00      0.00      0.00         1\n",
      "       04185       0.00      0.00      0.00         1\n",
      "         042       0.00      0.00      0.00         1\n",
      "       07054       0.00      0.00      0.00         2\n",
      "       07070       0.00      0.00      0.00         1\n",
      "        1120       0.00      0.00      0.00         1\n",
      "        1122       0.00      0.00      0.00         1\n",
      "        1623       0.00      0.00      0.00         1\n",
      "        1628       0.00      0.00      0.00         1\n",
      "        1970       0.00      0.00      0.00         1\n",
      "        1977       0.00      0.00      0.00         1\n",
      "        1983       0.00      0.00      0.00         1\n",
      "        1985       0.00      0.00      0.00         1\n",
      "       19889       0.00      0.00      0.00         1\n",
      "       20280       0.00      0.00      0.00         1\n",
      "        2449       0.00      0.00      0.00         8\n",
      "       25000       0.00      0.00      0.00        14\n",
      "       25001       0.00      0.00      0.00         1\n",
      "       25002       0.00      0.00      0.00         1\n",
      "       25013       0.00      0.00      0.00         1\n",
      "       25040       0.00      0.00      0.00         2\n",
      "       25041       0.00      0.00      0.00         1\n",
      "       25050       0.00      0.00      0.00         1\n",
      "       25060       0.00      0.00      0.00         2\n",
      "       25061       0.00      0.00      0.00         1\n",
      "       25080       0.00      0.00      0.00         1\n",
      "        2536       0.00      0.00      0.00         1\n",
      "        2639       0.00      0.00      0.00         2\n",
      "        2720       0.00      0.00      0.00         9\n",
      "        2724       0.00      0.00      0.00        13\n",
      "        2749       0.00      0.00      0.00         3\n",
      "        2753       0.00      0.00      0.00         1\n",
      "        2760       0.00      0.00      0.00         4\n",
      "        2761       0.00      0.00      0.00         5\n",
      "        2762       0.00      0.00      0.00         7\n",
      "        2763       0.00      0.00      0.00         2\n",
      "        2764       0.00      0.00      0.00         1\n",
      "        2765       0.00      0.00      0.00         2\n",
      "       27651       0.00      0.00      0.00         2\n",
      "       27652       0.00      0.00      0.00         2\n",
      "        2766       0.00      0.00      0.00         1\n",
      "        2767       0.00      0.00      0.00         3\n",
      "        2768       0.00      0.00      0.00         2\n",
      "       27800       0.00      0.00      0.00         2\n",
      "       27801       0.00      0.00      0.00         2\n",
      "        2800       0.00      0.00      0.00         2\n",
      "        2809       0.00      0.00      0.00         2\n",
      "        2841       0.00      0.00      0.00         1\n",
      "        2848       0.00      0.00      0.00         1\n",
      "        2851       0.00      0.00      0.00         7\n",
      "       28521       0.00      0.00      0.00         2\n",
      "       28529       0.00      0.00      0.00         2\n",
      "        2859       0.00      0.00      0.00         9\n",
      "        2866       0.00      0.00      0.00         1\n",
      "        2867       0.00      0.00      0.00         1\n",
      "        2869       0.00      0.00      0.00         2\n",
      "        2874       0.00      0.00      0.00         1\n",
      "        2875       0.00      0.00      0.00         5\n",
      "       28860       0.00      0.00      0.00         1\n",
      "       29181       0.00      0.00      0.00         1\n",
      "       29281       0.00      0.00      0.00         1\n",
      "        2930       0.00      0.00      0.00         2\n",
      "       29410       0.00      0.00      0.00         1\n",
      "        2948       0.00      0.00      0.00         2\n",
      "       29680       0.00      0.00      0.00         1\n",
      "       30000       0.00      0.00      0.00         2\n",
      "        3004       0.00      0.00      0.00         2\n",
      "       30390       0.00      0.00      0.00         1\n",
      "       30391       0.00      0.00      0.00         1\n",
      "       30500       0.00      0.00      0.00         1\n",
      "       30501       0.00      0.00      0.00         1\n",
      "        3051       0.00      0.00      0.00         5\n",
      "         311       0.00      0.00      0.00         5\n",
      "       32723       0.00      0.00      0.00         4\n",
      "        3310       0.00      0.00      0.00         1\n",
      "        3314       0.00      0.00      0.00         1\n",
      "        3320       0.00      0.00      0.00         1\n",
      "       33829       0.00      0.00      0.00         1\n",
      "         340       0.00      0.00      0.00         1\n",
      "       34290       0.00      0.00      0.00         1\n",
      "       34590       0.00      0.00      0.00         1\n",
      "       34690       0.00      0.00      0.00         1\n",
      "        3481       0.00      0.00      0.00         1\n",
      "       34830       0.00      0.00      0.00         1\n",
      "        3484       0.00      0.00      0.00         1\n",
      "        3485       0.00      0.00      0.00         2\n",
      "       34982       0.00      0.00      0.00         1\n",
      "        3569       0.00      0.00      0.00         1\n",
      "        3572       0.00      0.00      0.00         3\n",
      "       36201       0.00      0.00      0.00         1\n",
      "        3659       0.00      0.00      0.00         1\n",
      "        3962       0.00      0.00      0.00         1\n",
      "        3963       0.00      0.00      0.00         1\n",
      "        3970       0.00      0.00      0.00         1\n",
      "       39891       0.00      0.00      0.00         1\n",
      "        4011       0.00      0.00      0.00         1\n",
      "        4019       0.05      0.81      0.10        32\n",
      "       40390       0.00      0.00      0.00         5\n",
      "       40391       0.00      0.00      0.00         4\n",
      "       41011       0.00      0.00      0.00         1\n",
      "       41041       0.00      0.00      0.00         1\n",
      "       41071       0.00      0.00      0.00         5\n",
      "       41091       0.00      0.00      0.00         1\n",
      "        4111       0.00      0.00      0.00         3\n",
      "         412       0.00      0.00      0.00         5\n",
      "        4139       0.00      0.00      0.00         2\n",
      "       41400       0.00      0.00      0.00         2\n",
      "       41401       0.09      0.15      0.11        20\n",
      "       41402       0.00      0.00      0.00         1\n",
      "        4148       0.00      0.00      0.00         1\n",
      "       41519       0.00      0.00      0.00         1\n",
      "        4160       0.00      0.00      0.00         1\n",
      "        4168       0.00      0.00      0.00         3\n",
      "        4239       0.00      0.00      0.00         1\n",
      "        4240       0.00      0.00      0.00         5\n",
      "        4241       0.00      0.00      0.00         4\n",
      "        4254       0.00      0.00      0.00         3\n",
      "        4260       0.00      0.00      0.00         1\n",
      "        4271       0.00      0.00      0.00         3\n",
      "       42731       0.05      0.19      0.08        21\n",
      "       42732       0.00      0.00      0.00         2\n",
      "       42741       0.00      0.00      0.00         1\n",
      "        4275       0.00      0.00      0.00         2\n",
      "       42781       0.00      0.00      0.00         1\n",
      "       42789       0.00      0.00      0.00         4\n",
      "        4280       0.03      0.09      0.04        22\n",
      "       42820       0.00      0.00      0.00         1\n",
      "       42821       0.00      0.00      0.00         1\n",
      "       42822       0.00      0.00      0.00         2\n",
      "       42823       0.00      0.00      0.00         2\n",
      "       42830       0.00      0.00      0.00         1\n",
      "       42831       0.00      0.00      0.00         1\n",
      "       42832       0.00      0.00      0.00         2\n",
      "       42833       0.00      0.00      0.00         2\n",
      "       42843       0.00      0.00      0.00         1\n",
      "         430       0.00      0.00      0.00         1\n",
      "         431       0.00      0.00      0.00         2\n",
      "       43310       0.00      0.00      0.00         1\n",
      "       43411       0.00      0.00      0.00         1\n",
      "       43491       0.00      0.00      0.00         1\n",
      "       43820       0.00      0.00      0.00         1\n",
      "       43889       0.00      0.00      0.00         1\n",
      "       44020       0.00      0.00      0.00         1\n",
      "       44024       0.00      0.00      0.00         1\n",
      "        4412       0.00      0.00      0.00         1\n",
      "        4414       0.00      0.00      0.00         1\n",
      "        4439       0.00      0.00      0.00         2\n",
      "        4538       0.00      0.00      0.00         1\n",
      "       45620       0.00      0.00      0.00         1\n",
      "       45829       0.00      0.00      0.00         3\n",
      "        4589       0.00      0.00      0.00         3\n",
      "       45981       0.00      0.00      0.00         1\n",
      "        4821       0.00      0.00      0.00         1\n",
      "       48241       0.00      0.00      0.00         1\n",
      "         486       0.00      0.00      0.00         8\n",
      "       49121       0.00      0.00      0.00         2\n",
      "        4928       0.00      0.00      0.00         1\n",
      "       49320       0.00      0.00      0.00         1\n",
      "       49322       0.00      0.00      0.00         1\n",
      "       49390       0.00      0.00      0.00         3\n",
      "         496       0.00      0.00      0.00         7\n",
      "        5070       0.00      0.00      0.00         6\n",
      "        5119       0.00      0.00      0.00         5\n",
      "        5121       0.00      0.00      0.00         1\n",
      "         515       0.00      0.00      0.00         1\n",
      "        5180       0.00      0.00      0.00         4\n",
      "        5184       0.00      0.00      0.00         1\n",
      "        5185       0.00      0.00      0.00         3\n",
      "       51881       0.00      0.00      0.00        12\n",
      "       51882       0.00      0.00      0.00         1\n",
      "       51883       0.00      0.00      0.00         1\n",
      "       51884       0.00      0.00      0.00         1\n",
      "       51889       0.00      0.00      0.00         1\n",
      "       51919       0.00      0.00      0.00         1\n",
      "       53081       0.00      0.00      0.00        10\n",
      "       53085       0.00      0.00      0.00         1\n",
      "       53550       0.00      0.00      0.00         1\n",
      "        5363       0.00      0.00      0.00         1\n",
      "       53789       0.00      0.00      0.00         1\n",
      "        5533       0.00      0.00      0.00         1\n",
      "        5570       0.00      0.00      0.00         1\n",
      "        5601       0.00      0.00      0.00         2\n",
      "       56210       0.00      0.00      0.00         1\n",
      "       56400       0.00      0.00      0.00         1\n",
      "       56409       0.00      0.00      0.00         1\n",
      "         570       0.00      0.00      0.00         2\n",
      "        5712       0.00      0.00      0.00         2\n",
      "        5715       0.00      0.00      0.00         2\n",
      "        5722       0.00      0.00      0.00         1\n",
      "        5723       0.00      0.00      0.00         2\n",
      "        5761       0.00      0.00      0.00         1\n",
      "        5762       0.00      0.00      0.00         1\n",
      "        5770       0.00      0.00      0.00         2\n",
      "        5771       0.00      0.00      0.00         1\n",
      "        5781       0.00      0.00      0.00         1\n",
      "        5789       0.00      0.00      0.00         2\n",
      "       58381       0.00      0.00      0.00         1\n",
      "        5845       0.00      0.00      0.00         4\n",
      "        5849       0.00      0.00      0.00        14\n",
      "        5853       0.00      0.00      0.00         1\n",
      "        5854       0.00      0.00      0.00         1\n",
      "        5856       0.00      0.00      0.00         3\n",
      "        5859       0.00      0.00      0.00         5\n",
      "        5939       0.00      0.00      0.00         1\n",
      "        5990       0.00      0.00      0.00        11\n",
      "        5997       0.00      0.00      0.00         1\n",
      "       60000       0.00      0.00      0.00         2\n",
      "       60001       0.00      0.00      0.00         1\n",
      "        6822       0.00      0.00      0.00         1\n",
      "        6826       0.00      0.00      0.00         1\n",
      "        6930       0.00      0.00      0.00         1\n",
      "        7070       0.00      0.00      0.00         1\n",
      "       70703       0.00      0.00      0.00         2\n",
      "       70715       0.00      0.00      0.00         1\n",
      "       70722       0.00      0.00      0.00         1\n",
      "        7100       0.00      0.00      0.00         1\n",
      "        7140       0.00      0.00      0.00         1\n",
      "       71590       0.00      0.00      0.00         1\n",
      "        7242       0.00      0.00      0.00         1\n",
      "        7245       0.00      0.00      0.00         1\n",
      "       72888       0.00      0.00      0.00         1\n",
      "        7291       0.00      0.00      0.00         1\n",
      "       73300       0.00      0.00      0.00         3\n",
      "       73313       0.00      0.00      0.00         1\n",
      "        7455       0.00      0.00      0.00         1\n",
      "        7470       0.00      0.00      0.00         1\n",
      "       76516       0.00      0.00      0.00         1\n",
      "       76517       0.00      0.00      0.00         1\n",
      "       76518       0.00      0.00      0.00         1\n",
      "       76519       0.00      0.00      0.00         1\n",
      "       76525       0.00      0.00      0.00         1\n",
      "       76526       0.00      0.00      0.00         1\n",
      "       76527       0.00      0.00      0.00         1\n",
      "       76528       0.00      0.00      0.00         1\n",
      "         769       0.00      0.00      0.00         2\n",
      "        7706       0.00      0.00      0.00         1\n",
      "       77081       0.00      0.00      0.00         2\n",
      "       77089       0.00      0.00      0.00         1\n",
      "        7742       0.00      0.00      0.00         4\n",
      "        7746       0.00      0.00      0.00         1\n",
      "        7766       0.00      0.00      0.00         1\n",
      "        7793       0.00      0.00      0.00         2\n",
      "       77981       0.00      0.00      0.00         1\n",
      "       77989       0.00      0.00      0.00         1\n",
      "       78009       0.00      0.00      0.00         1\n",
      "       78039       0.00      0.00      0.00         3\n",
      "       78057       0.00      0.00      0.00         1\n",
      "        7806       0.00      0.00      0.00         1\n",
      "        7843       0.00      0.00      0.00         1\n",
      "        7850       0.00      0.00      0.00         1\n",
      "       78551       0.00      0.00      0.00         2\n",
      "       78552       0.00      0.00      0.00         4\n",
      "       78559       0.00      0.00      0.00         1\n",
      "        7863       0.00      0.00      0.00         1\n",
      "       78720       0.00      0.00      0.00         1\n",
      "       78791       0.00      0.00      0.00         2\n",
      "       78820       0.00      0.00      0.00         2\n",
      "       78830       0.00      0.00      0.00         1\n",
      "        7895       0.00      0.00      0.00         1\n",
      "       78959       0.00      0.00      0.00         1\n",
      "       79029       0.00      0.00      0.00         1\n",
      "        7907       0.00      0.00      0.00         3\n",
      "       79092       0.00      0.00      0.00         2\n",
      "       79902       0.00      0.00      0.00         2\n",
      "        8600       0.00      0.00      0.00         1\n",
      "       99591       0.00      0.00      0.00         2\n",
      "       99592       0.00      0.00      0.00         6\n",
      "       99662       0.00      0.00      0.00         2\n",
      "       99672       0.00      0.00      0.00         1\n",
      "       99674       0.00      0.00      0.00         1\n",
      "       99681       0.00      0.00      0.00         1\n",
      "       99702       0.00      0.00      0.00         1\n",
      "        9971       0.00      0.00      0.00         4\n",
      "        9973       0.00      0.00      0.00         1\n",
      "       99731       0.00      0.00      0.00         1\n",
      "        9974       0.00      0.00      0.00         1\n",
      "        9975       0.00      0.00      0.00         1\n",
      "       99811       0.00      0.00      0.00         3\n",
      "       99812       0.00      0.00      0.00         2\n",
      "        9982       0.00      0.00      0.00         2\n",
      "       99859       0.00      0.00      0.00         2\n",
      "       E8490       0.00      0.00      0.00         1\n",
      "       E8497       0.00      0.00      0.00         2\n",
      "       E8498       0.00      0.00      0.00         1\n",
      "       E8780       0.00      0.00      0.00         1\n",
      "       E8781       0.00      0.00      0.00         1\n",
      "       E8782       0.00      0.00      0.00         2\n",
      "       E8786       0.00      0.00      0.00         1\n",
      "       E8788       0.00      0.00      0.00         2\n",
      "       E8790       0.00      0.00      0.00         1\n",
      "       E8798       0.00      0.00      0.00         2\n",
      "       E8809       0.00      0.00      0.00         1\n",
      "       E8859       0.00      0.00      0.00         1\n",
      "       E8889       0.00      0.00      0.00         2\n",
      "       E9320       0.00      0.00      0.00         1\n",
      "       E9331       0.00      0.00      0.00         1\n",
      "       E9342       0.00      0.00      0.00         1\n",
      "       E9478       0.00      0.00      0.00         1\n",
      "        V053       0.00      0.00      0.00         3\n",
      "         V08       0.00      0.00      0.00         1\n",
      "        V090       0.00      0.00      0.00         1\n",
      "       V1005       0.00      0.00      0.00         1\n",
      "       V1011       0.00      0.00      0.00         1\n",
      "        V103       0.00      0.00      0.00         2\n",
      "       V1046       0.00      0.00      0.00         2\n",
      "       V1083       0.00      0.00      0.00         1\n",
      "       V1251       0.00      0.00      0.00         2\n",
      "       V1254       0.00      0.00      0.00         2\n",
      "       V1259       0.00      0.00      0.00         1\n",
      "        V153       0.00      0.00      0.00         1\n",
      "       V1581       0.00      0.00      0.00         1\n",
      "       V1582       0.00      0.00      0.00         4\n",
      "       V1588       0.00      0.00      0.00         1\n",
      "        V173       0.00      0.00      0.00         1\n",
      "        V290       0.17      1.00      0.29         4\n",
      "       V3000       0.00      0.00      0.00         2\n",
      "       V3001       0.00      0.00      0.00         2\n",
      "       V3101       0.00      0.00      0.00         1\n",
      "        V422       0.00      0.00      0.00         1\n",
      "        V433       0.00      0.00      0.00         1\n",
      "       V4364       0.00      0.00      0.00         1\n",
      "       V4365       0.00      0.00      0.00         1\n",
      "        V440       0.00      0.00      0.00         1\n",
      "        V441       0.00      0.00      0.00         1\n",
      "       V4501       0.00      0.00      0.00         2\n",
      "       V4502       0.00      0.00      0.00         1\n",
      "       V4511       0.00      0.00      0.00         1\n",
      "       V4581       0.00      0.00      0.00         5\n",
      "       V4582       0.00      0.00      0.00         4\n",
      "       V4589       0.00      0.00      0.00         1\n",
      "        V462       0.00      0.00      0.00         1\n",
      "       V4986       0.00      0.00      0.00         2\n",
      "        V502       0.00      0.00      0.00         1\n",
      "       V5861       0.00      0.00      0.00         6\n",
      "       V5865       0.00      0.00      0.00         1\n",
      "       V5867       0.00      0.00      0.00         4\n",
      "        V667       0.00      0.00      0.00         1\n",
      "        V707       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.05       764\n",
      "   macro avg       0.00      0.01      0.00       764\n",
      "weighted avg       0.01      0.05      0.01       764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_val = vectorizer.transform(val_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, random_state=SEED)\n",
    "clf.fit(X_train, train_df['label'])\n",
    "val_preds = clf.predict(X_val)\n",
    "test_preds = clf.predict(X_test)\n",
    "\n",
    "# Get the sorted list of labels present in the test set\n",
    "labels_in_test = sorted(test_df['label'].unique())\n",
    "target_names = [str(label2code[i]) for i in labels_in_test]\n",
    "\n",
    "print(classification_report(\n",
    "    test_df['label'],\n",
    "    test_preds,\n",
    "    labels=labels_in_test,\n",
    "    target_names=target_names\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76063b36",
   "metadata": {},
   "source": [
    "## 4. Transformer Models (BERT, ClinicalBERT, RoBERTa, Longformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a66c816f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c9e553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: emilyalsentzer/Bio_ClinicalBERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2441/2441 [00:14<00:00, 167.91 examples/s]\n",
      "Map: 100%|██████████| 611/611 [00:02<00:00, 288.42 examples/s]\n",
      "Map: 100%|██████████| 764/764 [00:02<00:00, 302.91 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='918' max='918' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [918/918 2:38:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.520100</td>\n",
       "      <td>5.495613</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.486800</td>\n",
       "      <td>5.441890</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.603000</td>\n",
       "      <td>5.424129</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='173' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [77/77 08:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation macro F1: 0.0003, Test macro F1: 0.0002\n",
      "Training model: roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2441/2441 [00:09<00:00, 270.44 examples/s]\n",
      "Map: 100%|██████████| 611/611 [00:02<00:00, 299.77 examples/s]\n",
      "Map: 100%|██████████| 764/764 [00:02<00:00, 311.07 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='918' max='918' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [918/918 2:26:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.498700</td>\n",
       "      <td>5.469385</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.464300</td>\n",
       "      <td>5.426568</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.622900</td>\n",
       "      <td>5.401266</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='173' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [77/77 05:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation macro F1: 0.0003, Test macro F1: 0.0002\n",
      "Training model: allenai/longformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2441/2441 [00:07<00:00, 323.99 examples/s]\n",
      "Map: 100%|██████████| 611/611 [00:01<00:00, 390.34 examples/s]\n",
      "Map: 100%|██████████| 764/764 [00:02<00:00, 370.71 examples/s]\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='374' max='918' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [374/918 5:38:59 < 8:15:43, 0.02 it/s, Epoch 1.22/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.540100</td>\n",
       "      <td>5.475090</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose model_name from: 'bert-base-uncased', 'emilyalsentzer/Bio_ClinicalBERT', 'roberta-base', 'allenai/longformer-base-4096'\n",
    "model_names = [\n",
    "    # 'bert-base-uncased',\n",
    "    'emilyalsentzer/Bio_ClinicalBERT',\n",
    "    'roberta-base',\n",
    "    'allenai/longformer-base-4096'\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for model_name in model_names:\n",
    "    print(f'Training model: {model_name}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=256)\n",
    "    train_ds = Dataset.from_pandas(train_df[['text', 'label']])\n",
    "    val_ds = Dataset.from_pandas(val_df[['text', 'label']])\n",
    "    test_ds = Dataset.from_pandas(test_df[['text', 'label']])\n",
    "    train_ds = train_ds.map(tokenize, batched=True)\n",
    "    val_ds = val_ds.map(tokenize, batched=True)\n",
    "    test_ds = test_ds.map(tokenize, batched=True)\n",
    "    train_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    val_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    test_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label2code))\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results_{model_name.replace(\"/\", \"_\")}',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='no',\n",
    "        learning_rate=2e-5,\n",
    "        logging_steps=10,\n",
    "        seed=SEED,\n",
    "        load_best_model_at_end=False,\n",
    "        metric_for_best_model='eval_loss',\n",
    "        report_to='none'\n",
    "    )\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        return {\n",
    "            'accuracy': accuracy_score(labels, preds),\n",
    "            'macro_f1': f1_score(labels, preds, average='macro')\n",
    "        }\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    trainer.train()\n",
    "    val_metrics = trainer.evaluate(val_ds)\n",
    "    test_metrics = trainer.evaluate(test_ds)\n",
    "    results[model_name] = {'val': val_metrics, 'test': test_metrics}\n",
    "    print(f\"Validation macro F1: {val_metrics['eval_macro_f1']:.4f}, Test macro F1: {test_metrics['eval_macro_f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844f0e13",
   "metadata": {},
   "source": [
    "## 5. Prompt Learning (Optional, OpenPrompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5840e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section requires OpenPrompt (pip install openprompt)\n",
    "try:\n",
    "    from openprompt.data_utils import InputExample\n",
    "    from openprompt.plms import load_plm\n",
    "    from openprompt.prompts import ManualTemplate, SoftTemplate, MixedTemplate\n",
    "    from openprompt.prompts import ManualVerbalizer, SoftVerbalizer\n",
    "    from openprompt import PromptForClassification, PromptDataLoader\n",
    "    import torch\n",
    "    # Prepare data for OpenPrompt\n",
    "    train_examples = [InputExample(text_a=row['text'], label=int(row['label'])) for _, row in train_df.iterrows()]\n",
    "    val_examples = [InputExample(text_a=row['text'], label=int(row['label'])) for _, row in val_df.iterrows()]\n",
    "    test_examples = [InputExample(text_a=row['text'], label=int(row['label'])) for _, row in test_df.iterrows()]\n",
    "    plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-uncased\")\n",
    "    template = MixedTemplate(model=plm, tokenizer=tokenizer, text=\"{'placeholder':'text_a'} It can be classified as {'mask'}.\")\n",
    "    verbalizer = SoftVerbalizer(tokenizer, plm, num_classes=len(label2code))\n",
    "    prompt_model = PromptForClassification(plm=plm, template=template, verbalizer=verbalizer)\n",
    "    # DataLoader\n",
    "    train_dataloader = PromptDataLoader(dataset=train_examples, template=template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=256, batch_size=8, shuffle=True, teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\")\n",
    "    val_dataloader = PromptDataLoader(dataset=val_examples, template=template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=256, batch_size=8, shuffle=False, teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\")\n",
    "    # Training loop (simplified)\n",
    "    optimizer = torch.optim.AdamW(prompt_model.parameters(), lr=2e-5)\n",
    "    for epoch in range(3):\n",
    "        prompt_model.train()\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = prompt_model(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1} done.\")\n",
    "    # Validation\n",
    "    prompt_model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    for batch in val_dataloader:\n",
    "        logits = prompt_model(batch)\n",
    "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        labels = batch['label'].cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "    print('Prompt Learning Validation F1:', f1_score(all_labels, all_preds, average='macro'))\n",
    "except ImportError:\n",
    "    print('OpenPrompt not installed. Skipping prompt learning section.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3079f57e",
   "metadata": {},
   "source": [
    "## 6. Knowledge Injection (ICD Descriptions, Synonyms, Hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load ICD-9/ICD-10 code descriptions, synonyms, and hierarchy\n",
    "# Example: Use code descriptions as additional features or for retrieval-augmented generation (RAG)\n",
    "# For demonstration, we'll just print a placeholder\n",
    "print('Add ICD-9/ICD-10 code descriptions, synonyms, or hierarchy as features or for RAG here.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd139c",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e778796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Try different learning rates, batch sizes, and dropout rates for the best transformer model\n",
    "# You can use optuna, Ray Tune, or manual search\n",
    "# For demonstration, we'll just print a placeholder\n",
    "print('Implement hyperparameter search for best model performance here.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838429c",
   "metadata": {},
   "source": [
    "## 8. Evaluation and Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7672b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model on test set\n",
    "# Show classification report, confusion matrix, and per-code F1\n",
    "# Optionally, visualize attention weights or use LIME/SHAP for explainability\n",
    "print('Evaluate the best model and add explainability tools here.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b50fae6",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "- This notebook demonstrated ICD code classification from MIMIC-III clinical notes using baseline and advanced neural NLP models.\n",
    "- You can extend it with more data, more advanced models (e.g., NoteContrast, GKI-ICD), or more explainability tools.\n",
    "- For ICD-10 tasks, map ICD-9 codes to ICD-10 using an external mapping.\n",
    "- For production, consider using larger datasets, more compute, and domain-specific pretraining.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
